{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing gradient\n",
    "\n",
    "Training DNNs requires computing gradient of the loss with respect to the parameters, and this is topic of this notebook.\n",
    "\n",
    "Your task will be to:\n",
    "\n",
    "* Implement Evolution Search training of DNNs that does not require backpropagation\n",
    "* Implement forward and backward functions for Linear and ReQU modules\n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Introduce ingredients necessary to train NN:\n",
    "    * model\n",
    "    * loss\n",
    "    * optimizer\n",
    "* Introduce different ways of computing or estimating $\\frac{\\partial{L(w)}}{{\\partial w}}$\n",
    "* Get understanding of Evolution Search and Backpropagation algorithms\n",
    "\n",
    "Exam:\n",
    "\n",
    "* For exam you are expected to understand how backpropagation works. Questions might refer to implementation details in PyTorch. See also Resources section for a good video about backpropgation, and how it is implemented in PyTorch.\n",
    "\n",
    "What's (probably) next:\n",
    "\n",
    "* Go through each component of the training loop:\n",
    "    * (Lab 5) Neural Networks practical, part 1: architecture, understanding what is learned\n",
    "    * (Lab 6, ?) Neural Networks practical part 2: setting up data and loss\n",
    "    * (Lab 7, ?) Neural Networks practical part 3: optimization\n",
    "\n",
    "Resources:\n",
    "\n",
    "* Backprop with focus on PyTorch: https://www.youtube.com/watch?v=ma2KXWblllc (see also other lectures from this series)\n",
    "\n",
    "* Evolution Search https://eng.uber.com/deep-neuroevolution/, https://arxiv.org/abs/1712.06564\n",
    "\n",
    "* Matrix derivatives http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD requires $\\frac{\\partial L (w) }{\\partial w}$!\n",
    "\n",
    "<p>\n",
    "<font size=5>\n",
    "$$\\frac{\\partial L (w) }{\\partial w} = \\frac{1}{K} \\sum_{i=1}^{K} \\frac{\\partial L (x_i, w) }{\\partial w}$$\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<img width=600 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/3/sgd.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from torch.autograd import gradcheck\n",
    "\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Some helper functions\n",
    "\n",
    "def get_model_weights(model):\n",
    "    params = model.state_dict()\n",
    "    p_order = [p[0] for p in sorted(model.named_parameters())]\n",
    "    return np.concatenate([params[w].cpu().numpy().reshape(-1, ) for w in p_order])\n",
    "    \n",
    "def set_model_weights(model, w):\n",
    "    params = model.state_dict()  \n",
    "    p_order = [p[0] for p in sorted(model.named_parameters())]\n",
    "    id = 0\n",
    "    for p in p_order:\n",
    "        shape = params[p].shape\n",
    "        D = np.prod(shape)\n",
    "        params[p].copy_(torch.from_numpy(w[id:id + D].reshape(shape)))\n",
    "        id += D\n",
    "        \n",
    "def L(w, loss, model, batch_size=10, x_tr=x_train, y_tr=y_train):\n",
    "    set_model_weights(model, w)\n",
    "    cost = 0.\n",
    "    n_examples, n_features = x_tr.size()\n",
    "    num_batches = n_examples // batch_size\n",
    "    for k in range(num_batches): \n",
    "        start, end = k * batch_size, (k + 1) * batch_size\n",
    "        x = Variable(x_tr[start:end], requires_grad=False)\n",
    "        y = Variable(y_tr[start:end], requires_grad=False)\n",
    "        fx = model.forward(x)\n",
    "        cost += loss.forward(fx, y)\n",
    "    return cost.data.numpy()[0] / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop in PyTorch\n",
    "\n",
    "In this section we walk through implementation of a traditional training loop  in PyTorch.\n",
    "\n",
    "Necessary ingredients to train a neural network are:\n",
    "    * model\n",
    "    * loss\n",
    "    * optimizer\n",
    "    \n",
    "<img width=400 src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/4/smoothie.png\">\n",
    "\n",
    "Ref:\n",
    "\n",
    "https://github.com/vinhkhuc/PyTorch-Mini-Tutorials/blob/master/3_neural_net.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_logreg(input_dim, output_dim, weight_module=torch.nn.Linear):\n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_2\", weight_module(input_dim, output_dim, bias=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model(input_dim, output_dim, nonlinearity=torch.nn.functional.sigmoid, weight_module=torch.nn.Linear):\n",
    "    class Nonlinearity(torch.nn.Module):\n",
    "        def forward(self, input):\n",
    "            return nonlinearity(input)\n",
    "    \n",
    "    model = torch.nn.Sequential()\n",
    "    model.add_module(\"linear_1\", weight_module(input_dim, 512, bias=False))\n",
    "    model.add_module(\"nonlinearity_1\", Nonlinearity())\n",
    "    model.add_module(\"linear_2\", weight_module(512, output_dim, bias=False))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_step(model, loss, x_val, y_val, lr=0.1, momentum=0.9):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    \n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    y = Variable(y_val, requires_grad=False)\n",
    "\n",
    "    # Reset gradient\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward\n",
    "    fx = model.forward(x)\n",
    "    output = loss.forward(fx, y)\n",
    "\n",
    "    # Backward\n",
    "    output.backward(retain_graph=True)\n",
    "\n",
    "    # Update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model, x_val):\n",
    "    x = Variable(x_val, requires_grad=False)\n",
    "    output = model.forward(x)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(model, loss, step_fnc):\n",
    "    torch.manual_seed(42)\n",
    "    n_examples, n_features = x_train.size()\n",
    "    n_classes = 10\n",
    "    batch_size = 100\n",
    "    history = []\n",
    "    for i in tqdm.tqdm(range(100), total=100):\n",
    "        cost = 0.\n",
    "        num_batches = n_examples // batch_size\n",
    "        for k in range(num_batches):\n",
    "            start, end = k * batch_size, (k + 1) * batch_size\n",
    "            cost += step_fnc(model, loss, x_train[start:end], y_train[start:end])\n",
    "        predY = predict(model, x_test)\n",
    "        print((\"Epoch %d, cost = %f, acc = %.2f%%\"\n",
    "              % (i + 1, cost / num_batches, 100. * np.mean(predY == y_test.numpy()))))\n",
    "        \n",
    "        history.append(cost)\n",
    "        \n",
    "    return np.mean(predY == y_test.numpy()), history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model(n_features, n_classes)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating $\\frac{\\partial L (w) }{\\partial w}$\n",
    "\n",
    "In this section we will implement different gradient computation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite Difference\n",
    "<p>\n",
    "<font size=4>\n",
    "Assuming we know direction we want to optimize in, we can approximate derivative simply by calculating\n",
    "\n",
    "$$\\langle \\frac{\\partial L (w) }{\\partial w}, \\vec{v} \\rangle \\approx \\frac{L(w) - L(w + v)}{|\\vec{v}|}$$\n",
    "\n",
    "\n",
    ", where $L(w)$ denotes esimation of loss (e.g. over whole training set, over $5\\%$ etc.). \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "As we will see later PyTorch uses Finite Difference method in gradient checks of automatic diffentiation modules.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution Search\n",
    "\n",
    "<p>\n",
    "Intuitively, Evolution Search (ES) approximates gradient by computing multiple times finite difference. Importantly it works for non-differentiable costs as well.\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$ \\frac{\\partial L (w) }{\\partial w} \\approx \\frac{1}{N \\sigma} \\sum \\vec{v} L(w + \\vec{v})$$\n",
    "\n",
    "\n",
    ", where $\\epsilon$ is a normally distributed vector sampled from gauss of standard deviation $\\sigma$, and $L(w)$ denotes esimation of loss (e.g. over whole training set, over $5\\%$ etc.). \n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Note: while it is recommended to not use minibatching in ES, for simplicity we will.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "### Variance reduction trick\n",
    "\n",
    "Evolution Search works better (lower variance) if reward is standardized. A simple way to achieve it is to introduce a baseline:\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$ \\frac{\\partial L (w) }{\\partial w} \\approx \\frac{1}{N \\sigma} \\sum \\vec{v} ( L(w + \\vec{v}) - L(w) )$$\n",
    "\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Note that it has (as a random variable over random vectors) the same mean, but a different variance, compared to the previous expression.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Evolution Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ES_grad(model, loss, x_tr, y_tr, sigma=0.001, N=100):\n",
    "    \"\"\"\n",
    "    Estimate dL/dw using Evolution Search.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    model: torch.nn.Model\n",
    "        Model to take optimization step on\n",
    "    loss: torch.nn.Module\n",
    "        Loss function to optimize\n",
    "    x_tr: torch.Tensor, (n_examples, n_features)\n",
    "        Batch to compute gradient over\n",
    "    y_tr: torch.Tensor, (n_examples, )\n",
    "        Batch to compute gradient over\n",
    "    sigma: float\n",
    "    \n",
    "    N: int\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    g: np.vector, size: (D, )\n",
    "        Gradient of loss with respect to model's weights\n",
    "    \"\"\"\n",
    "    grad = 0\n",
    "    init_w = get_model_weights(model)\n",
    "    rng = np.random.RandomState()\n",
    "    base_loss = L(w=init_w, model=model, loss=loss, x_tr=x_tr, y_tr=y_tr)\n",
    "    for _ in tqdm.tqdm(range(N), total=N):\n",
    "        v = ?? # Hint: use rng\n",
    "        reward = ?? # Hint: use L\n",
    "        grad += ?? # Hint: see equation\n",
    "    return ?? # Hint: normalize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If code is slow, you can use build_logreg !\n",
    "# model = build_logreg(784, 10) \n",
    "model = build_model(784, 10)\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "init_w = get_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "down = 0\n",
    "for _ in range(10):\n",
    "    loss_before = L(w=init_w, model=model, loss=loss)\n",
    "    set_model_weights(model=model, w=init_w)\n",
    "    v = ES_grad(model, loss, x_tr=x_train, y_tr=y_train, sigma=0.002, N=50)\n",
    "    loss_after = L(w=init_w - 0.1*v, model=model, loss=loss)\n",
    "    if loss_before > loss_after:\n",
    "        down += 1\n",
    "        print(\"Went down!\")\n",
    "print(\"{}/{}\".format(down, 10))\n",
    "# This code is stochastic, so of course sometimes this assert can fail. \n",
    "assert down > 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement own module\n",
    "\n",
    "PyTorch enable defining own Modules. Here we will implement Linear and ReQu modules ourselves, to understand better how backpropagation works. \n",
    "\n",
    "Below you can find an example Module implementing the classial ReLU nonlinearity.\n",
    "\n",
    "Ref: https://github.com/jcjohnson/pytorch-examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyReLU(torch.autograd.Function):\n",
    "  \"\"\"\n",
    "  We can implement our own custom autograd Functions by subclassing\n",
    "  torch.autograd.Function and implementing the forward and backward passes\n",
    "  which operate on Tensors.\n",
    "  \"\"\"\n",
    "  def forward(self, input):\n",
    "    \"\"\"\n",
    "    In the forward pass we receive a Tensor containing the input and return a\n",
    "    Tensor containing the output. You can cache arbitrary Tensors for use in the\n",
    "    backward pass using the save_for_backward method.\n",
    "    \"\"\"\n",
    "    self.save_for_backward(input)\n",
    "    return input.clamp(min=0)\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    grad_input = grad_output.clone()\n",
    "    grad_input[input < 0] = 0\n",
    "    return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Gradient check passes!\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "assert(gradcheck(MyReLU(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(n_features, n_classes, nonlinearity=MyReLU())\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "_, H_relu = train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement “ReQU” unit\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "Here, we’ll implement a made-up activation function that we’ll call the Rectified Quadratic Unit\n",
    "(ReQU). Like the sigmoid and ReLU and several others, it is applied element-wise to all its\n",
    "inputs:\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "<font size=4>\n",
    "$$z_i = I[x_i > 0] x_i ^ 2$$\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "(Note, exercise is taken from  https://www.cs.ox.ac.uk/people/nando.defreitas/machinelearning/practicals/practical4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: use ReLU implementation as a template\n",
    "\n",
    "class ReQU(torch.autograd.Function):\n",
    "  def forward(self, input):\n",
    "    self.save_for_backward(input)\n",
    "    ??\n",
    "\n",
    "  def backward(self, grad_output):\n",
    "    \"\"\"\n",
    "    In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "    with respect to the output, and we need to compute the gradient of the loss\n",
    "    with respect to the input.\n",
    "    \"\"\"\n",
    "    input, = self.saved_tensors\n",
    "    ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## GradCheck\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "assert(gradcheck(ReQU(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = build_model(n_features, n_classes, nonlinearity=ReQU())\n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "_, H_requ = train(step_fnc=sgd_step, loss=loss, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Comparing ReLU and ReQU\n",
    "plt.plot(H_relu, label=\"ReLU\")\n",
    "plt.plot(H_requ, label=\"ReQU\")\n",
    "plt.ylim([0, 10])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement W*x + b Function\n",
    "\n",
    "(Code from PyTorch official tutorial)\n",
    "\n",
    "#### Crash course on matrix derivatives\n",
    "\n",
    "If A and B are matrices with elements being functions, then by (dA/dB)_{v1, v2} we understand dA[v1]/dB[v2], where v1 and v2 are two tuples of indices. Assuming this definition, chain rule applies to matrix operations, similarly as it would to normal functions.\n",
    "\n",
    "Example useful for the exercise: A = WX + b (i.e. $A_i = \\sum w_ik x_k + b_i$), then $\\frac{\\partial A}{\\partial W}$ is simply $W$.\n",
    "\n",
    "Hint: in the exercise you will have to consider dimension with respect to examples, which is ignored in this example (i.e. we assume here A is 1 dimensional, and X is 1 dimensional as well).\n",
    "\n",
    "Refs:\n",
    "\n",
    "* Matrix Cookbook: http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Linear(torch.autograd.Function):\n",
    "    def forward(self, input, weight, bias=None):\n",
    "        # Hint: implement forward and check it manually, before implementing backward\n",
    "        self.save_for_backward(input, weight, bias)\n",
    "        output = ??\n",
    "        if bias is not None:\n",
    "            output += ??\n",
    "        return output\n",
    "\n",
    "    # This function has only a single output, so it gets only one gradient\n",
    "    def backward(self, grad_output):\n",
    "        input, weight, bias = self.saved_tensors\n",
    "        grad_input = grad_weight = grad_bias = None\n",
    "\n",
    "        # Hint1: grad should have same shape as variable it is taken wrt to\n",
    "        # Example: grad_bias should be 1 dimensional, because bias is a vectors\n",
    "        \n",
    "        # Hint2: you can use print, or even pdb inside backward pass\n",
    "        # for example you can use print(grad_output.shape) \n",
    "        # Why is grad_output (20, 15)?\n",
    "        \n",
    "        # YOU DON'T HAVE TO IMPLEMENT GRAD_INPUT :)\n",
    "\n",
    "        grad_weight = ??\n",
    "        grad_bias = ?? \n",
    "\n",
    "        # dL/dinput_i !, where i=0 is input, i=1 is weight, i=2 is bias\n",
    "        return None, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hint: if implementing bias gradient is difficult, consider removing from input triple last item\n",
    "# this will test only gradient wrt to input and weight\n",
    "input = (Variable(torch.randn(20,20).double(), requires_grad=False),  # Input\n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True), # Weight\n",
    "         Variable(torch.randn(15,).double(), requires_grad=True)) # Bias\n",
    "test = gradcheck(Linear(), input, eps=1e-6, atol=1e-4)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = (Variable(torch.randn(20,20).double(), requires_grad=True),)\n",
    "result['requ'] = 0.5*int(gradcheck(ReQU(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = (Variable(torch.randn(20,20).double(), requires_grad=False), \n",
    "         Variable(torch.randn(15,20).double(), requires_grad=True),\n",
    "         Variable(torch.randn(15,).double(), requires_grad=True))\n",
    "result['linear'] = 0.5*int(gradcheck(Linear(), input, eps=1e-6, atol=1e-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If code is slow, you can use build_logreg !\n",
    "model = build_logreg(784, 10) \n",
    "loss = torch.nn.CrossEntropyLoss(size_average=True)\n",
    "init_w = get_model_weights(model)\n",
    "down = 0\n",
    "for _ in range(10):\n",
    "    loss_before = L(w=init_w, model=model, loss=loss)\n",
    "    set_model_weights(model=model, w=init_w)\n",
    "    v = ES_grad(model, loss, x_tr=x_train, y_tr=y_train, sigma=0.002, N=50)\n",
    "    loss_after = L(w=init_w - 0.1*v, model=model, loss=loss)\n",
    "    if loss_before > loss_after:\n",
    "        down += 1\n",
    "# This code is stochastic, so of course sometimes this assert can fail. \n",
    "result['ES'] = int(down > 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(result, open(\"4a_computing_gradient.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
