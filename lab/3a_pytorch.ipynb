{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mulitnomial Logistic Regression in PyTorch\n",
    "\n",
    "Your task will be to:\n",
    "\n",
    "* Implement and optimize multinomial logistic regression using Pytorch (with and without Autograd)\n",
    "\n",
    "Goal is to:\n",
    "\n",
    "* Get accustomed with the environment\n",
    "* Learn basics of Pytorch\n",
    "* Brush up on Machine Learning\n",
    "\n",
    "Next week:\n",
    "\n",
    "* Implementing own backprop (using PyTorch forward/backward abstraction)\n",
    "* Using nn module in PyTorch\n",
    "* Implementing a full neural network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['figure.figsize'] = (7, 7)\n",
    "mpl.rcParams['axes.titlesize'] = 12\n",
    "mpl.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get FashionMNIST (see 1b_FMNIST.ipynb for data exploration)\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Logistic regression needs 2D data\n",
    "x_train = x_train.reshape(-1, 784)\n",
    "x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "# 0-1 normalization\n",
    "x_train = x_train / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "# For simplicity - implicit bias\n",
    "x_train = np.concatenate([np.ones_like(x_train[:, 0:1]), x_train], axis=1)\n",
    "x_test = np.concatenate([np.ones_like(x_test[:, 0:1]), x_test], axis=1)\n",
    "\n",
    "# Convert to Torch Tensor. Just to avoid boilerplate code later\n",
    "x_train = torch.from_numpy(x_train).type(torch.FloatTensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.FloatTensor)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor)\n",
    "\n",
    "# Use only first 1k examples. Just for notebook to run faster\n",
    "x_train, y_train = x_train[0:1000], y_train[0:1000]\n",
    "x_test, y_test = x_test[0:1000], y_test[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why not Numpy\n",
    "\n",
    "Ref: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf\n",
    "\n",
    "<img src=\"https://github.com/gmum/nn2018/raw/master/lab/fig/2/GPUvsCPU.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch mini tutorial\n",
    "\n",
    "Ref: http://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 1: numpy-like features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# This is like numpy!\n",
    "a = torch.IntTensor([2, 3, 4])\n",
    "b = torch.IntTensor([3, 4, 5])\n",
    "m = a * b  # element-wise product\n",
    "print(m.numpy())  # convert to the numpy array [ 6 12 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2363  0.6124  0.0408\n",
      " 0.0193  0.0747  0.9665\n",
      " 0.3959  0.1364  0.2696\n",
      " 0.4519  0.7188  0.3281\n",
      " 0.2082  0.4813  0.2480\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.1656  0.8504  1.0193\n",
      " 0.7813  0.2592  1.9063\n",
      " 0.7980  0.3430  1.1790\n",
      " 0.6872  0.9283  0.3537\n",
      " 0.8426  0.5105  0.7863\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(5, 3)\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.1656  0.8504  1.0193\n",
      " 0.7813  0.2592  1.9063\n",
      " 0.7980  0.3430  1.1790\n",
      " 0.6872  0.9283  0.3537\n",
      " 0.8426  0.5105  0.7863\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.add(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.6124\n",
      " 0.0747\n",
      " 0.1364\n",
      " 0.7188\n",
      " 0.4813\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Just like numpy! Amazing!\n",
    "print(x[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    x + y\n",
    "else:\n",
    "    print(\"No GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level 2: Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.AddConstantBackward object at 0x11c1c0718>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      ", Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad) # dout/dx :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial logistic regression\n",
    "\n",
    "(Using PyTorch)\n",
    "\n",
    "## Useful equations\n",
    "\n",
    "Let $D$ denote input dimension, $K$ denote number of classes. Class probabilities are given by the softmax distribution:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ p(\\textbf{y} = k | \\textbf{x}, \\textbf{w}) = \\frac{\\exp(\\langle\\textbf{w}_k, \\textbf{x}\\rangle)}{\\sum_i^{K} \\exp(\\langle\\textbf{w}_i, \\textbf{x}\\rangle)}$\n",
    "</font>\n",
    "\n",
    ", where $w \\in \\mathbb{R}^{(D,K)}$, $w_k$ denotes $k^{th}$ row of the $w$ matrix. For $K=2$ equivalent to the binomial logistic regression discussed in the previous notebook:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ p(\\textbf{y} | \\textbf{x}, \\textbf{w}) = \\sigma(\\langle\\textbf{w}, \\textbf{x}\\rangle)$\n",
    "</font>\n",
    "\n",
    "Same as in binary case, loss is the cross-entropy loss:\n",
    "\n",
    "<br>\n",
    "<font size=4>\n",
    "$ L(\\textbf{w}) = \\sum_i y_i \\log(p(\\textbf{y_i} | \\textbf{x_i}, \\textbf{w}))$\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement helper functions \n",
    "\n",
    "Hint: very similar to 2a_logistic_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_pred, y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: torch.Variable, shape: (batch_size, K)\n",
    "        Probabiities\n",
    "    y: torch.Variable, shape: (batch_size,)\n",
    "        Correct classes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    loss: torch.Variable, shape: (1, )\n",
    "        Cross entropy loss\n",
    "    \"\"\"\n",
    "    # Hint: use y as indexes to retrieve appropriate columns of y_pred\n",
    "    ??\n",
    "\n",
    "def softmax(h):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    h: torch.Variable, shape: (batch_size, K)\n",
    "        Logits\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred: torch.Variable, shape: (batch_size, K)\n",
    "        Softmax on h\n",
    "    \"\"\"\n",
    "    ??\n",
    "\n",
    "def forward(x, w):\n",
    "    return softmax(x.mm(w))\n",
    "\n",
    "def evaluate(w):\n",
    "    x = Variable(x_test)\n",
    "    y_test_pred = forward(x, w).data.numpy()\n",
    "    return np.mean(y_test_pred.argmax(axis=1) == y_test.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b5fa86e7a1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# No grad wrt to data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Grad wrt to weights!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0myours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'softmax' is not defined"
     ]
    }
   ],
   "source": [
    "# Asserts to help you implement the functions.\n",
    "D = 785\n",
    "K = 10\n",
    "\n",
    "x = Variable(x_train[0:100], requires_grad=False) # No grad wrt to data\n",
    "y = Variable(y_train[0:100], requires_grad=False) # No grad wrt to data \n",
    "w = Variable(torch.randn(D, K), requires_grad=True) # Grad wrt to weights!\n",
    "y_pred = softmax(x.mm(w))\n",
    "\n",
    "yours = cross_entropy_loss(y_pred, y).data[0]\n",
    "correct = nn.CrossEntropyLoss()(x.mm(w), y).data[0]\n",
    "assert np.allclose(yours, correct, atol=0.01)\n",
    "\n",
    "yours = softmax(x.mm(w)).data.numpy()\n",
    "correct = F.softmax(x.mm(w)).data.numpy()\n",
    "assert np.allclose(yours, correct, atol=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(lr=0.1, n_epochs=100, batch_size=100):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learning rate used in SGD\n",
    "    n_epochs: int\n",
    "        Number of epochs to train\n",
    "    use_autograd: bool\n",
    "        If true will use PyTorch autograd\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w: np.array, shape: (D_in, D_out)\n",
    "        Found parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    n_epochs = 100\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.1\n",
    "    use_autograd = True\n",
    "\n",
    "    # 784 + bias -> 10 model\n",
    "    D, K = 784 + 1, 10 \n",
    "\n",
    "    # Define all Variables used in the computation\n",
    "    dtype = torch.FloatTensor\n",
    "    x = Variable(torch.randn(batch_size, D).type(dtype), requires_grad=False) # No grad wrt to data\n",
    "    y = Variable(torch.randn(batch_size, K).type(dtype), requires_grad=False) # No grad wrt to data\n",
    "    w = Variable(torch.randn(D, K).type(dtype), requires_grad=True) # Grad wrt to weights!\n",
    "\n",
    "    loss_history = []\n",
    "    for epoch in tqdm.tqdm(range(n_epochs), total=n_epochs):    \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            # Sample data\n",
    "            x_batch = x_train[batch*batch_size:(batch+1)*batch_size]\n",
    "            y_batch = y_train[batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "            x_batch = Variable(x_batch.type(torch.FloatTensor), requires_grad=False)\n",
    "            y_batch = Variable(y_batch.type(torch.LongTensor), requires_grad=False)\n",
    "\n",
    "            y_pred = ?? # Hint: use forward\n",
    "            loss = ?? # Hint: cross_entropy_loss\n",
    "\n",
    "            if batch == 0:\n",
    "                loss_history.append(loss.data.numpy())\n",
    "\n",
    "            # Compute grad_w\n",
    "            grad_w = ?? # Hint: use w.grad and backward. Google if needed.\n",
    "\n",
    "            # Update weights using gradient descent\n",
    "            w.data -= # Hint: use learning rate and computed gradient\n",
    "\n",
    "            ?? # You need to zero gradients, before next batch.  Google if needed.\n",
    "\n",
    "    plt.plot(loss_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "Each notebook will terminate with Tests section. This will automatically grade and assign points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results:\n",
      "===========\n",
      "{'test1': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAb0AAAGwCAYAAAA9sLuaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xmc5HV95/H3p66uvu/puadnYGZk\nAijQchhQI2iIIYEYo+ARD5Q8srtBNBse6j42bk6zm8Rg1NVMxEhWg4kEVzwWAUEwCsgM9wzMMAwz\n3T1H3/d9fPeP+lV3z9Dd02f9qur7ej4e9aiqX/2m6jNlOW++39/3MOecAADwQSTsAgAAyBRCDwDg\nDUIPAOANQg8A4A1CDwDgDUIPAOANQg8A4A1CDwDgDUIPAOCNWNgFLFZNTY2rr68PuwwAQBbZu3dv\nu3Ou9kzn5Vzo1dfXa8+ePWGXAQDIImZ2dCHn0b0JAPAGoQcA8AahBwDwBqEHAPAGoQcA8AahBwDw\nBqEHAPAGoQcA8AahBwDwBqEHAPAGoQcA8AahBwDwBqEHAPAGoQcA8EbObS20XE2dg3rwxVatryjU\nW3fVhV0OACCDvGvpHTjZp8/cs093/qIx7FIAABnmXejFY6m/8tjEZMiVAAAyzbvQS0RTf+XRcUIP\nAHzjX+jFTJI0SksPALzjXejFo3RvAoCvvAu9RPqa3rgLuRIAQKZ5F3rplh7dmwDgH+9Cj4EsAOAv\n/0IvRksPAHzlXegxkAUA/OVd6E0PZCH0AMA33oVePMo8PQDwlX+hF0l3bzo5x7QFAPCJd6EXidhU\na29sgtADAJ94F3oSc/UAwFdehx6DWQDAL16GXoLthQDAS36GXtDSG6GlBwBeyUjomdnXzKzVzJ6f\n5bU/NDNnZjWZqEXSjIEshB4A+CRTLb2vS7r69INmtknS2yQ1ZqgOSTO7Nxm9CQA+yUjoOecekdQ5\ny0t/J+lWSRlNnziLTgOAl0K7pmdm10o65px7ZgHn3mRme8xsT1tb27I/m0WnAcBPoYSemRVJ+rSk\nP17I+c653c65BudcQ21t7bI/n0WnAcBPYbX0zpK0VdIzZnZE0kZJT5rZ2kx8OHvqAYCfYmF8qHPu\nOUlr0s+D4GtwzrVn4vOZpwcAfsrUlIU7JT0qaaeZNZvZjZn43LlM7bRASw8AvJKRlp5z7oYzvF6f\niTrSWHsTAPzk54oszNMDAC/5GXoMZAEAL3kZekxZAAA/eRl6jN4EAD95GXpxdlkAAC95GXoJdlkA\nAC/5GXp0bwKAl7wMPXZZAAA/eR16zNMDAL94GXpsLQQAfvIz9OjeBAAv+Rl6DGQBAC95GXqsyAIA\nfvI09NhaCAB85GXoTQ9kYfQmAPjEz9BLd2/S0gMAr3gZenGmLACAl7wMvQQDWQDAS16GHsuQAYCf\nvAy9RCwYvUlLDwC84mfoRaOS6N4EAN94GXrxGPP0AMBHfoYeuywAgJe8DL2ptTdp6QGAV/wMvaCl\nN8I1PQDwipehN3PBaefo4gQAX3gZetGIKRoxOSdNTBJ6AOALL0NPmrHTAl2cAOANb0NvetFpWnoA\n4At/Q49FpwHAO96G3tT6m4QeAHjD29Bjrh4A+Mfb0IuzvRAAeMf70BuhpQcA3vA29Ka6N2npAYA3\n/A29KDstAIBvvA09dloAAP94G3p0bwKAf7wNPQayAIB/vA09WnoA4J+MhJ6Zfc3MWs3s+RnH/trM\nXjSzZ83sO2ZWkYla0hLM0wMA72Sqpfd1SVefdux+Sec6586XdFDSpzJUi6QZuyzQvQkA3shI6Dnn\nHpHUedqx+5xz48HTxyRtzEQtaXRvAoB/suWa3ocl/b9MfuD0gtNMWQAAX4Qeemb23ySNS/rmPOfc\nZGZ7zGxPW1vbinxu+poe3ZsA4I9QQ8/MPijpGknvdc7N2eRyzu12zjU45xpqa2tX5LPp3gQA/8TC\n+mAzu1rSrZLe5JwbzPTns8sCAPgnU1MW7pT0qKSdZtZsZjdK+qKkUkn3m9nTZvaVTNSSFqd7EwC8\nk5GWnnPuhlkO356Jz55LunuTndMBwB+hD2QJS3qXBbo3AcAf3oYe3ZsA4B9vQ2969Cbz9ADAF96G\nHi09APCPt6HHQBYA8I+/occ8PQDwjrehR/cmAPjH29BjGTIA8I+3oTe1nx6jNwHAGx6HHt2bAOAb\nb0OvgO5NAPCOt6HHLgsA4B9/Qy9G9yYA+Mbb0GOeHgD4x/vQo6UHAP7wNvTisfSUBUIPAHzhbehN\nd28yTw8AfOFt6EUjJjNpYtJpYpLgAwAfeBt6Zsa0BQDwjLehJ0kFUbYXAgCfeB166bl6Y4zgBAAv\neB16CVp6AOAVr0MvPW1hbJyBLADgA79Db6qlNxFyJQCATPA69KZXZaGlBwA+8Dv02F4IALzidejF\nGcgCAF7xOvSmliJjygIAeMHr0JvaU4+WHgB4wevQS0SDnRZo6QGAF/wOvRg7LQCAT7wOPRacBgC/\nEHqiexMAfOF16CUYyAIAXvE79OjeBACv+B16Mbo3AcAnXodePJiyQEsPAPzgeeilr+kxZQEAfOB1\n6NG9CQB+8Tv0GMgCAF7xOvSYpwcAfslI6JnZ18ys1cyen3GsyszuN7OXgvvKTNQyE/vpAYBfMtXS\n+7qkq0879klJP3bObZf04+B5RrGfHgD4JSOh55x7RFLnaYevlXRH8PgOSddlopaZ4uyyAABeCfOa\nXp1z7kTw+KSkurlONLObzGyPme1pa2tbsQIK6N4EAK9kxUAW55yTNOdkOefcbudcg3Ouoba2dsU+\nd3qXBebpAYAPwgy9FjNbJ0nBfWumC2D0JgD4JczQu0fSB4LHH5D03UwXwC4LAOCXTE1ZuFPSo5J2\nmlmzmd0o6a8kvdXMXpJ0VfA8o9hEFgD8EsvEhzjnbpjjpSsz8flzKWAZMgDwSlYMZAkLLT0A8Ivn\noZfeWojRmwDgA69Dj10WAMAvfocey5ABgFe8Dj2u6QGAX7wOPbo3AcAvXoceLT0A8IvnoTc9enNy\nkhGcAJDvvA49M5sazDI2SWsPAPKd16EnMVcPAHxC6DGYBQC84X3oJRjMAgDe8D702FMPAPzhfeix\npx4A+IPQo3sTALzhfejFY8HozXFGbwJAvvM+9KYXnZ4IuRIAwGrzPvSmB7LQ0gOAfOd96KUHsnBN\nDwDyn/ehV5yISZJ6h8dCrgQAsNq8D72a0oQkqb1vJORKAACrzfvQqy1JSpLa+0dDrgQAsNoIvdIC\nSVIbLT0AyHveh15NSap7s62f0AOAfOd96KVbeu2EHgDkPUKP7k0A8Ib3oVdTMt3Sc44J6gCQz7wP\nvWQ8qtJkTGMTTj1DzNUDgHzmfehJUm0JXZwA4ANCT1JN+roeg1kAIK8RemIwCwD4gtDTdPcmq7IA\nQH4j9ERLDwB8QehpxqoshB4A5DVCT6zKAgC+IPQ0vdMCLT0AyG+Enqb31GPKAgDkN0JPUnVxqnuz\nc2BUE5MsRQYA+WrBoWdmnzCz1wWPLzWzRjN7xcwuW73yMiMRi6iiKK6JSaeuQaYtAEC+WkxL7+OS\nXgkef1bS5yT9uaTbVrqoMNSWMJgFAPLdYkKv3DnXY2alkl4r6QvOudsl7VxOAWb2cTPbZ2bPm9md\nZpZczvstFXP1ACD/LSb0mszsDZKul/SIc27CzMokTSz1w81sg6SbJTU4586VFA3eP+NqWHQaAPJe\nbBHn/pGkuySNSvrt4Ng1kn6xAjUUmtmYpCJJx5f5fkvCXD0AyH8LDj3n3A8lrT/t8LeD25I4546Z\n2d9IapQ0JOk+59x9S32/5aClBwD5bzGjN3eZWV3wuMTM/kTSpyXFl/rhZlYp6VpJW5UK1GIze98s\n591kZnvMbE9bW9tSP25eXNMDgPy3mGt6d0qqCB7/jaQ3SrpU0j8s4/OvkvSKc67NOTcm6W5Jbzj9\nJOfcbudcg3Ouoba2dhkfN7fp7k2mLABAvlrMNb1659wBMzNJ75C0S6kuyVfm/2PzapR0qZkVBe91\npaQ9y3i/JWPRaQDIf4sJveFgusIuSY3OuXYzi0la8hQD59zjZnaXpCcljUt6StLupb7fcjCQBQDy\n32JC718kPSipVNIXg2MXanktPTnnPiPpM8t5j5VQVZSQmdQ5OKqxiUnFo6zQBgD5ZjGjNz9uZm+T\nNOaceyg4PKnUSi05LxaNqLo4ofb+UXUOjKquLJQ58gCAVbSo5kwwneBlM7vMzDY75/Y45x5cpdoy\njmkLAJDfFjNlYZ2ZPSzpJaVGWR4ys4fN7PS5ezlratoC1/UAIC8tpqX3ZUnPSKpyzq2TVCnpaUlf\nWY3CwjC16DQtPQDIS4sZyHK5pHXBfDo55wbM7FZJx1alshDU0NIDgLy2mJZel1LTFWbaKal75coJ\nVy3X9AAgry2mpfe/JD1gZrdLOippi6QPSfrvq1FYGFiVBQDy24Jbes65f5T0bkk1kn4juH+PpI2r\nU1rmTY/eHA65EgDAalhMS0/B9ISpKQpmViDpPkl/vMJ1hYJFpwEgv63EsiO2Au+RFdaWpyakH+8e\nlnMu5GoAACttJUIvb9KhvDCuyqK4hsYmaO0BQB46Y/emmb1lnpcTK1hLVthcXayuwW4d6RjUGpYi\nA4C8spBreref4fXGlSgkW2ypKtIzTd062jGgi7dWhV0OAGAFnTH0nHNbM1FItqivLpIkNXYOhlwJ\nAGClsX/OaTZXF0uSjnQQegCQbwi902xJt/Q6BkKuBACw0gi906RD7yjdmwCQdwi909SWFKgoEVX3\n4Jh6BsfCLgcAsIIIvdOYmTZXpVt7dHECQD4h9GYx1cXJYBYAyCuE3iy2BCM4jzKYBQDyCqE3i6nu\nTVp6AJBXCL1Z1KdbeozgBIC8QujNYnquHqEHAPmE0JvFuvKkYhHTyd5hDY9NhF0OAGCFEHqziEUj\n2lhZKIk1OAEgnxB6c5gewUnoAUC+IPTmMD1Xj2kLAJAvCL05MG0BAPIPoTcHpi0AQP4h9OZA9yYA\n5B9Cbw6bgu7NY11DGp+YDLkaAMBKIPTmkIxHta48qfFJp+Pdw2GXAwBYAYTePNKDWY7QxQkAeYHQ\nmwfX9QAgvxB689hWWyJJermN0AOAfEDozWPn2lJJ0osne0OuBACwEgi9eeysS4XegZN9cs6FXA0A\nYLkIvXmsK0+qNBlT1+CY2vtHwy4HALBMhN48zOyU1h4AILcRemewI7iud6CF0AOAXBd66JlZhZnd\nZWYvmtkLZnZZ2DXN9Jog9A7S0gOAnBcLuwBJn5d0r3PunWaWkFQUdkEz7Qi6N1+kpQcAOS/U0DOz\ncklvlPRBSXLOjUrKqhEj6Wt6L7X0aXLSKRKxkCsCACxV2N2bWyW1SfonM3vKzL5qZsUh13SKyuKE\n1pQWaHB0Qs1dQ2GXAwBYhrBDLybpQklfds5dIGlA0idPP8nMbjKzPWa2p62tLdM1Tk1SZzALAOS2\nsEOvWVKzc+7x4PldSoXgKZxzu51zDc65htra2owWKE1f1ztI6AFATgs19JxzJyU1mdnO4NCVkvaH\nWNKsppcjI/QAIJdlw+jNP5D0zWDk5mFJHwq5nldJD2Zh2gIA5LbQQ88597SkhrDrmM/2uhKZSS+3\n9Wt0fFKJWNi9wgCApeBf7wUoSsS0uapI45NOr7SzzRAA5CpCb4HSg1kYwQkAuYvQWyCWIwOA3Efo\nLdDUcmSEHgDkLEJvgdLTFpirBwC5i9BboK01xYpHTY2dg+obHgu7HADAEhB6CxSPRrRrfbkk6Zmm\nnpCrAQAsBaG3CBdtrpQk7T3aFXIlAIClIPQW4cItFZKkJxsJPQDIRYTeIly0JdXSe7KxS5OTLuRq\nAACLRegtwrryQq0vT6pveFyH2vrDLgcAsEiE3iJduIXregCQqwi9RbqI0AOAnEXoLdKFwQjOJwk9\nAMg5hN4i7VpfpmQ8osPtA+ocGA27HADAIhB6ixSPRnT+xtTUhaeYugAAOYXQWwKu6wFAbiL0loCV\nWQAgNxF6S3DB5lT35jPN3RqbmAy5GgDAQhF6S1BdUqCtNcUaHpvUCyd6wy4HALBAhN4SMXUBAHIP\nobdE6cEsjx7uCLkSAMBCEXpL9KadtZKkRw62a3hsIuRqAAALQegt0YaKQp23oVxDYxP6j5fawy4H\nALAAhN4yvG1XnSTpvv0nQ64EALAQhN4yvO2X1kqSHnihVRPsrwcAWY/QW4YddSXaUl2kzoFR7TnS\nGXY5AIAzIPSWwcz0q0Fr7779LSFXAwA4E0JvmWZe13OOLk4AyGaE3jJdsLlSNSUJNXUO6YUTfWGX\nAwCYB6G3TNGI6a2M4gSAnEDorYC37Qqu6+3juh4AZDNCbwVcdla1ihNR7T/Rq8aOwbDLAQDMgdBb\nAcl4dKqL89t7m0KuBgAwF0JvhVx/8WZJ0r8+0aRx9tgDgKxE6K2QS7ZWaVttsVr7RvTgi61hlwMA\nmAWht0LMTO8JWnv/8ovGkKsBAMyG0FtB77hwoxLRiB4+2KbmLga0AEC2IfRWUFVxQlefu1bOSf/2\nBANaACDbEHor7D2XBANa9jCgBQCyDaG3wi7ZWqVtNcVq6WVACwBkm6wIPTOLmtlTZvb9sGtZLjPT\nDcGAlm88zoAWAMgmWRF6kj4m6YWwi1gp77xoowrjUT1ysE3PNneHXQ4AIBB66JnZRkm/LumrYdey\nUiqLE3r/ZVskSX//45dCrgYAkBZ66Em6TdKtkuYc9WFmN5nZHjPb09bWlrnKluGjV2xTMh7RAy+0\n6vljPWGXAwBQyKFnZtdIanXO7Z3vPOfcbudcg3Ouoba2NkPVLU9taYHedwmtPQDIJmG39H5Z0m+a\n2RFJ35L0FjP7RrglrZyb3rRNBbGI7tvfov3He8MuBwC8F2roOec+5Zzb6Jyrl3S9pAedc+8Ls6aV\ntKY0qffS2gOArBF2Sy/v/d6btikRi+jefSdp7QFAyLIm9JxzP3HOXRN2HSutriyp9wartPyP7+2T\ncy7kigDAX1kTevnslit3qLo4oV+80qnvPHUs7HIAwFuEXgaUF8X1qbefI0n6yx++oJ6hsZArAgA/\nEXoZ8tsXbtDF9VVq7x/V3953IOxyAMBLhF6GmJn+7LpzFY2Y/s9jR/VcMxPWASDTCL0M2rm2VDde\nvlXOSZ/+znMaY+shAMgoQi/DPnbldq0vT+q5Yz267YGDYZcDAF4h9DKsuCCmv3v36xQx6X//5GX9\n/OX2sEsCAG8QeiG4ZFu1/suvnC3npE/86zPqGhgNuyQA8AKhF5Kbr9yui7ZU6mTvsG7992eZtA4A\nGUDohSQWjejz179OpcmY7t/fon/62ZGwSwKAvEfohWhjZZH+6h3nS5L+/Af79cD+lpArAoD8RuiF\n7NfPX6ePX7VDk076gzufYv4eAKwiQi8L3Hzl2frtCzdqaGxCH77jCTV3DYZdEgDkJUIvC5iZPvuO\n83TZtmq19Y3ow19/Qh39I2GXBQB5h9DLEolYRF95/0XavqZEB1v6df3ux9TSOxx2WQCQVwi9LFJe\nGNc3P3qJdtaV6qXWfr3rHx6lqxMAVhChl2XWlCb1rZsu1XkbynW0Y1Dv+sqjOtzWH3ZZAJAXCL0s\nVFmc0Dc/eokatlTqeM+wrvvSz/STA61hlwUAOY/Qy1Jlybj++caLddU5deodHteHvv6EvvTQIVZu\nAYBlIPSyWFEipt3vv0gfv2qHnJP++kcH9PvfeJKd1wFgiQi9LBeJmD521Xbd/oEGlRbEdO++k/q1\n2x7Rzw+xOwMALBahlyOuPKdO9/zB5XrtxnId7xnWe776uP70e/s1PDYRdmkAkDMIvRyytaZYd/3+\nG3TLVdsVjZi+9rNX9Pa//6keO9wRdmkAkBMIvRwTj0Z0y1U79J3/9AadvaZEh9sGdP3ux/RH32Zf\nPgA4E0IvR52/sUI/uPlyfeKtO5SIRvTtvc16y9/+RHf+olETk4zwBIDZEHo5rCAW1c1Xbte9t1yh\nN5xVra7BMX3q7uf0G1/4D7o8AWAWhF4e2FZbom9+5BJ94YYLtL48qf0nenX97sf0+9/Yy2ouADCD\n5dpk54aGBrdnz56wy8haQ6MT2v3IYX354UMaHptUNGJ6V8Mm3XLVdtWVJcMuDwBWhZntdc41nPE8\nQi8/negZ0m33v6Rv723SpJOS8Yg+cFm9bnrjNlWXFIRdHgCsKEIPkqRDrX366x8d0I/2tUiSCuNR\n/e4btuimKwg/APmD0MMpnmnq1ud//JIefDG1cHVhPKobLt6sj1yxVesrCkOuDgCWh9DDrJ5p6tZt\nDxzUQwfaJEmxiOm6Czbo9964TdvrSkOuDgCWhtDDvPYd79E/PHxY33/2uNLT+q7YXqMPX75Vb9pe\nq0jEwi0QABaB0MOCNHYM6h9/elh37W3WULCO57baYr3/0i16x4UbVV4YD7lCADgzQg+L0j04qm89\n0aQ7fn5EJ3qGJaWu+/3ma9frPZds1vkby2VG6w9AdiL0sCRjE5N6YH+LvvH4Uf3s0PSqLjvrSvU7\nDRv1WxdsYNQngKxD6GHZDrf1618eb9TdTx1TZ7CYdSxievPOWl37ug266pw6FSaiIVcJAIQeVtDo\n+KQefLFF397TrIcOtE4NfClORPXWXXV6+3nr9MYdtUrGCUAA4SD0sCpa+4b1g2dP6LtPH9fTTd1T\nx4sSUf3Ka9boV39prd60o5YBMAAyitDDqjvSPqAfPHdC9z5/Us8d65k6HouYXl9fpSvPWaM371yj\ns2qLGQQDYFXlROiZ2SZJ/yypTpKTtNs59/n5/gyhl52aOgf1o30ndf/+Fu052nXKnn4bKgr1xh01\numJ7rS7bVq3K4kSIlQLIR7kSeuskrXPOPWlmpZL2SrrOObd/rj9D6GW/nsEx/eRgqx56sVU/fald\nHTN2dDeTzllbpl8+u1qXbqtWQ30VXaEAli0nQu90ZvZdSV90zt0/1zmEXm6ZnHTad7xXDx9s1c8O\ndWjv0S6NTkxOvW4m7VpXpou3VqlhS5Ua6ivZAgnAouVc6JlZvaRHJJ3rnOud6zxCL7cNj01o79Eu\n/fzldj1+uFPPNHdrbOLU3+CmqkJdtLlSF2yu1IWbK/WadaWKR9nvGMDccir0zKxE0sOS/sI5d/cs\nr98k6SZJ2rx580VHjx7NcIVYLUOjE3qqsUtPHOnSnqOdeqqxW/0j46ecUxCL6NwN5Xrtxgq9dlO5\nzt9YoS1VRawPCmBKzoSemcUlfV/Sj5xznzvT+bT08tvEpNOLJ3v1VGO3nmzs0tON3TrcPvCq80qT\nMZ23oVznbSjXL20o17nry1RfXUwQAp7KidCz1Dj2OyR1OuduWcifIfT80z04qmebe/RMU7eeburW\ns8d61NY38qrzihNRnbOuTLvWl2nXujKds65MO9eWMmke8ECuhN7lkn4q6TlJ6dENn3bO/XCuP0Po\nQZJaeof1XHOPnj3Wo/3He7TveO/UQtkzRUyqrynWOWvL9Jq1pdoZ3DZV0j0K5JOcCL2lIPQwl/b+\nEb1wolf7j/dqf3B/uH3glDmDaYXxqLbXlWj7mlLtqCvRjrWl2r6mROvLCwlDIAcReoBSo0UPtfbr\nwMk+vXCiVwda+nSwpU8tva/uHpVSy6mdvaZEZ9eW6Kw1JTqrtkRnrynRluoiRpACWWyhoRfLRDFA\nWJLxqM7dUK5zN5Sfcrx7cFQHW/p1MAjBgy19OtQ6oPb+ET3b3KNnm3tOOT8WMW2uLtJZtSXaVlus\nbTXF2lZboq01xaouTrDMGpAjCD14qaIooYu3VunirVWnHO8eHNVLrf061Nqvl1v7dagt9fhY95AO\ntw3ocNurR5KWJWPaWlOsrTXFqg/u04/Lkqw2A2QTQg+YoaIoodfXV+n19aeG4fDYhF5pH9DLbf16\nuXVAr7T363B7KgR7h8f1THOPnjmtdShJ1cUJbakuUn11sbZUF2tLdZE2B88ri+K0EIEMI/SABUjG\nU9MhzllXdspx55za+0f1SvuAjrQP6HBwf6QjdesYGFXHwKiebOx+1XuWFsS0ubpIm6uKpu+D2/qK\nQq4hAquA0AOWwcxUW1qg2tKCV3WVTk46tfaN6EjHgI52DOhIx6AaOwZ1pGNAjR2D6hsZ177jvdp3\n/NWr7kUjpnXlSW2qTIXgpqpCbaxM3W+qLFJNSQGjTIElIPSAVRKJmNaWJ7W2PKlLt1Wf8ppzTl2D\nYzraMaDGzkEd7RhUU+egGjtT9yd6h9XcNaTmriE9erjjVe+diEW0saJQGypTYbixslAbKwu1oSL1\nfE0poQjMhtADQmBmqipOqKo4oQs2V77q9ZHxCR3rGlJT15CaOgfV1DWo5s4hNXWlQrFrcCx1TXGW\nJdokKR41rStPheCGykKtryjUhoqkNlQUaX1FUusrClmpBl4i9IAsVBCLalttibbVlsz6+sDIuI51\npwLxWPdQ0CocVHPXkI51DaljYFSNQctxLtXFCa2rSGp9eSoU15Unta6iUOuD+7rSAsW4rog8Q+gB\nOai4IKYddaXaUVc66+tDoxM63pMKwGPdQzrePeNxz5BO9gxPDbJ5/tjsO3lFTFpTmuqeXV+R1Nqy\nVDCmu2zXliVVV5ZUIkYwIncQekAeKkxEdVZtakWZ2UxOOrX1j+h495BO9AzrePeQjncP60TPkI73\nDOtE95Da+kd0sndYJ3uH9XTT3J9VU5JQXVlS68pTIbi2LKm6mY/LClReyPQMZAdCD/BQJGKqC1pq\nF8xxzuj4pFqC0DvePaSW3mGd6BnWie7UsZbg1t4/qvb+0VlHoaYVxCLB5xVoTVlSdaVJrSkrUF1Z\nwdTjNWVJlRbECEesKkIPwKwSsYg2VRVpU1XRnOdMTDq19QUtwp5hnewZUkvfiFp6hqdaia29I+of\nGT/jNUZJSsYjqi0t0JrSpNaUFqRuZUnVlhSotqxAtSWpY9UlBYoyOhVLQOgBWLLojGkZ2jT3ef0j\n42oJArC1L91KHFFr34haeofVFtwPjk6oqXNITZ1D835uxKSq4oRqSlJzJGtLClQzdZ86nn6tsihB\nQGIKoQdg1ZUUxFQyzzXGtP6RcbX2Dqu1b0RtfSMz7lPBmL51Do5Odau+eLJv3vecGZCpW0LVJQWq\nLklMPy9OPa8uLlBhgqkc+Ywnf1BrAAAJUUlEQVTQA5A10uE411SNtPGJSXUOjE6FYlv/iNr7U4/b\n+0fV3hc87x9R9+DYVEBK8weklNpeqrokoariAlUXJ1RdnFBVSXCfPlaSUGVR6r4owT+juYT/tQDk\nnFg0ojVlSa0pS57x3LEgINuCIOzoH1XHQCocpx8Hx/tHNTg6ocEFdLGmJeMRVRWlgrGyKDG16EBV\nUUKVweOKovjUsYqiBNM8QkToAchr8WhkaqTqmTjn1D8yPhWGqftRdQ6kArFzYGTqeVcwz3F4bFLH\ne4Z1vGd4wTWVFMSmgrCiKKHKorgqi1KhWVEUV8VpzyuLEypORBnZugIIPQAImJlKk3GVJuOqryk+\n4/nOOQ2OTqgzCMLOwVF19o+qa3B06ljX4Ki6BsamjnUPjal/ZFz9I+Nq7lpYa1JKLS1XXhiEYFF8\n6nFFYSoky4sS048L46ooTKi8MK7SZIx1WGcg9ABgicxMxQUxFRfE5p3aMdPkpFPfyLi6gpDsnhGK\n3YOn3ncNjqknuB8am1B7cO1ycTVKZcn4VECWFaZCca5b2Yz70oL8C0xCDwAyKBKxqYCp15lbk2nD\nYxPqGRo7JRh7htIBOaaeoTH1DqVeS5/XOzSmvpFx9QylXj+6yFrNUvs+TgVhMq6ywtjU49Lgeep4\nXGXJ1LllQQuzJJF9oUnoAUAOSMajSsajC7o2OdP4xKR6h8fVPZjqWk2HY8/QmHqCsJx56x0en3q9\nf2Q89Xx4cV2xaWap65epgIxNhWZp8Lw0GdPmqiK9+/WbF/3eS0XoAUAei0UjUyNKF2t8YlJ9w+Pq\nHU6FYN/wdKuxb3hMvUPj6pvx2szzeofGNDA6ob7hcfUNj8/5GedtKCf0AADhi0UjqixOTb1YivGJ\nSfWPjE+FZSoApwOyb3h8SWG8HIQeAGBVxKIRVQRzE+dZpS6jmCEJAPAGoQcA8AahBwDwBqEHAPAG\noQcA8AahBwDwBqEHAPAGoQcA8AahBwDwBqEHAPAGoQcA8AahBwDwBqEHAPAGoQcA8AahBwDwhjnn\nwq5hUcysTdLRFXirGkntK/A++YjvZn58P/Pj+5kf38/clvPdbHHO1Z7ppJwLvZViZnuccw1h15GN\n+G7mx/czP76f+fH9zC0T3w3dmwAAbxB6AABv+Bx6u8MuIIvx3cyP72d+fD/z4/uZ26p/N95e0wMA\n+Mfnlh4AwDPehZ6ZXW1mB8zskJl9Mux6wmZmm8zsITPbb2b7zOxjwfEqM7vfzF4K7ivDrjUsZhY1\ns6fM7PvB861m9njwG/pXM0uEXWNYzKzCzO4ysxfN7AUzu4zfzjQz+3jw/6vnzexOM0v6/Psxs6+Z\nWauZPT/j2Ky/F0v5++B7etbMLlyJGrwKPTOLSvqSpF+TtEvSDWa2K9yqQjcu6Q+dc7skXSrpPwff\nyScl/dg5t13Sj4PnvvqYpBdmPP+fkv7OOXe2pC5JN4ZSVXb4vKR7nXOvkfRapb4nfjuSzGyDpJsl\nNTjnzpUUlXS9/P79fF3S1acdm+v38muStge3myR9eSUK8Cr0JF0s6ZBz7rBzblTStyRdG3JNoXLO\nnXDOPRk87lPqH60NSn0vdwSn3SHpunAqDJeZbZT065K+Gjw3SW+RdFdwis/fTbmkN0q6XZKcc6PO\nuW7x25kpJqnQzGKSiiSdkMe/H+fcI5I6Tzs81+/lWkn/7FIek1RhZuuWW4NvobdBUtOM583BMUgy\ns3pJF0h6XFKdc+5E8NJJSXUhlRW22yTdKmkyeF4tqds5Nx489/k3tFVSm6R/Crp/v2pmxeK3I0ly\nzh2T9DeSGpUKux5Je8Xv53Rz/V5W5d9r30IPczCzEkn/LukW51zvzNdcaoivd8N8zewaSa3Oub1h\n15KlYpIulPRl59wFkgZ0Wlemr78dSQquTV2r1H8crJdUrFd37WGGTPxefAu9Y5I2zXi+MTjmNTOL\nKxV433TO3R0cbkl3JQT3rWHVF6JflvSbZnZEqa7wtyh1Dasi6K6S/P4NNUtqds49Hjy/S6kQ5LeT\ncpWkV5xzbc65MUl3K/Wb4vdzqrl+L6vy77VvofeEpO3B6KmEUheV7wm5plAF16hul/SCc+5zM166\nR9IHgscfkPTdTNcWNufcp5xzG51z9Ur9Vh50zr1X0kOS3hmc5uV3I0nOuZOSmsxsZ3DoSkn7xW8n\nrVHSpWZWFPz/LP398Ps51Vy/l3sk/W4wivNSST0zukGXzLvJ6Wb2dqWu00Qlfc059xchlxQqM7tc\n0k8lPafp61afVuq63r9J2qzUrhbvcs6dfgHaG2b2Zkn/1Tl3jZltU6rlVyXpKUnvc86NhFlfWMzs\ndUoN8klIOizpQ0r9xzS/HUlm9ieS3q3UKOmnJH1EqetSXv5+zOxOSW9WajeFFkmfkfR/NcvvJfgP\nhS8q1SU8KOlDzrk9y67Bt9ADAPjLt+5NAIDHCD0AgDcIPQCANwg9AIA3CD0AgDcIPcADZubM7Oyw\n6wDCRugBITCzI2Y2ZGb9M25fDLsuIN/FznwKgFXyG865B8IuAvAJLT0gi5jZB83sZ2b2RTPrCTZn\nvXLG6+vN7B4z6ww21/zojNeiZvZpM3vZzPrMbK+ZzVy78Kpgo85uM/tSsOIF4BVaekD2uUSpxZtr\nJL1D0t1mtjVYyutbkp5XatX+10i638xeds49KOkTkm6Q9HZJByWdr9TyTWnXSHq9pDKltrj5nqR7\nM/I3ArIEy5ABIQh2bqhRak3GtD+SNCbpLyVtCLZZkZn9QtIXJP1E0hFJFcGGvzKzz0pa55z7oJkd\nkHSrc+5VCxibmZN0hXPuP4Ln/ybpSefcX63KXxDIUnRvAuG5zjlXMeP2j8HxY+7U/xo9qlTLbr2k\nznTgzXgtvbHmJkkvz/N5J2c8HpRUsrzygdxD6AHZZ8Np19s2Szoe3KrMrPS019J7jDVJOiszJQK5\nidADss8aSTebWdzMfkfSOZJ+6JxrkvRzSZ81s6SZnS/pRknfCP7cVyX9mZltD/YgO9/MqkP5GwBZ\nioEsQHi+Z2YTM57fr9QGmo9L2i6pXak9x97pnOsIzrlB0leUavV1SfrMjGkPn5NUIOk+pa4Xvijp\nt1b7LwHkEgayAFnEzD4o6SPOucvDrgXIR3RvAgC8QegBALxB9yYAwBu09AAA3iD0AADeIPQAAN4g\n9AAA3iD0AADeIPQAAN74/6hJyQJlLdJZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11afc6e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = {}\n",
    "result['test1'] = int(evaluate(train_logistic_regression(n_epochs=100, lr=0.1)) > 0.6)\n",
    "print(\"Evaluation results:\\n===========\")\n",
    "print(result)\n",
    "json.dump(result, open(\"2b_pytorch.json\", \"w\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
